{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Simplicial High-Skip Network (HSN)\n",
    "\n",
    "In this notebook, we will create and train a High Skip Network in the simplicial complex domain, as proposed in the paper by [Hajij et. al : High Skip Networks: A Higher Order Generalization of Skip Connections (2022)](https://openreview.net/pdf?id=Sc8glB-k6e9). \n",
    "\n",
    "We train the model to perform binary node classification using the KarateClub benchmark dataset. \n",
    "\n",
    "The equations of one layer of this neural network are given by:\n",
    "\n",
    "游린 $\\quad m_{{y \\rightarrow z}}^{(0 \\rightarrow 0)} = \\sigma ((A_{\\uparrow,0})_{xy} \\cdot h^{t,(0)}_y \\cdot \\Theta^{t,(0)1})$    (level 1)\n",
    "\n",
    "游린 $\\quad m_{z \\rightarrow x}^{(0 \\rightarrow 0)}  = (A_{\\uparrow,0})_{xy} \\cdot m_{y \\rightarrow z}^{(0 \\rightarrow 0)} \\cdot \\Theta^{t,(0)2}$    (level 2)\n",
    "\n",
    "游린 $\\quad m_{{y \\rightarrow z}}^{(0 \\rightarrow 1)}  = \\sigma((B_1^T)_{zy} \\cdot h_y^{t,(0)} \\cdot \\Theta^{t,(0 \\rightarrow 1)})$    (level 1)\n",
    "\n",
    "游린 $\\quad m_{z \\rightarrow x)}^{(1 \\rightarrow 0)}  = (B_1)_{xz} \\cdot m_{z \\rightarrow x}^{(0 \\rightarrow 1)} \\cdot \\Theta^{t, (1 \\rightarrow 0)}$    (level 2)\n",
    "\n",
    "游릲 $\\quad m_{x}^{(0 \\rightarrow 0)}  = \\sum_{z \\in \\mathcal{L}_\\uparrow(x)} m_{z \\rightarrow x}^{(0 \\rightarrow 0)}$\n",
    "\n",
    "游릲 $\\quad m_{x}^{(1 \\rightarrow 0)}  = \\sum_{z \\in \\mathcal{C}(x)} m_{z \\rightarrow x}^{(1 \\rightarrow 0)}$\n",
    "\n",
    "游릴 $\\quad m_x^{(0)}  = m_x^{(0 \\rightarrow 0)} + m_x^{(1 \\rightarrow 0)}$\n",
    "\n",
    "游릱 $\\quad h_x^{t+1,(0)}  = I(m_x^{(0)})$\n",
    "\n",
    "Where the notations are defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from toponetx import SimplicialComplex\n",
    "import toponetx.datasets.graph as graph\n",
    "\n",
    "from topomodelx.nn.simplicial.hsn_layer import HSNLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import dataset ##\n",
    "\n",
    "The first step is to import the Karate Club (https://www.jstor.org/stable/3629752) dataset. This is a singular graph with 34 nodes that belong to two different social groups. We will use these groups for the task of node-level binary classification.\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicial Complex with shape (34, 78, 45, 11, 2) and dimension 4\n"
     ]
    }
   ],
   "source": [
    "dataset = graph.karate_club(complex_type=\"simplicial\", feat_dim=8)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures. ##\n",
    "\n",
    "Now we retrieve the neighborhood structures (i.e. their representative matrices) that we will use to send messges on the domain. In this case, we need the boundary matrix (or incidence matrix) $B_1$ and the adjacency matrix $A_{\\uparrow,0}$ on the nodes. For a santiy check, we show that the shape of the $B_1 = n_\\text{nodes} \\times n_\\text{edges}$ and $A_{\\uparrow,0} = n_\\text{nodes} \\times n_\\text{nodes}$. We also convert the neighborhood structures to torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The incidence matrix B1 has shape: torch.Size([34, 78]).\n",
      "The adjacency matrix A0 has shape: torch.Size([34, 34]).\n"
     ]
    }
   ],
   "source": [
    "incidence_1 = dataset.incidence_matrix(rank=1)\n",
    "adjacency_0 = dataset.adjacency_matrix(rank=0)\n",
    "\n",
    "incidence_1 = torch.from_numpy(incidence_1.todense()).to_sparse()\n",
    "adjacency_0 = torch.from_numpy(adjacency_0.todense()).to_sparse()\n",
    "\n",
    "print(f\"The incidence matrix B1 has shape: {incidence_1.shape}.\")\n",
    "print(f\"The adjacency matrix A0 has shape: {adjacency_0.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import signal ##\n",
    "\n",
    "Since our task will be node classification, we must retrieve an input signal on the nodes. The signal will have shape $n_\\text{nodes} \\times$ in_channels, where in_channels is the dimension of each cell's feature. Here, we have in_channels = channels_nodes $ = 34$. This is because the Karate dataset encodes the identity of each of the 34 nodes as a one hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = []\n",
    "for _, v in dataset.get_simplex_attributes(\"node_feat\").items():\n",
    "    x_0.append(v)\n",
    "x_0 = torch.tensor(np.stack(x_0))\n",
    "channels_nodes = x_0.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 nodes with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_0.shape[0]} nodes with features of dimension {x_0.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load edge features, this is how we would do it (note that we will not use these features for this model, and this serves simply as a demonstration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"edge_feat\").items():\n",
    "    x_1.append(v)\n",
    "x_1 = np.stack(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 78 edges with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_1.shape[0]} edges with features of dimension {x_1.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for face features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"face_feat\").items():\n",
    "    x_2.append(v)\n",
    "x_2 = np.stack(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 45 faces with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_2.shape[0]} faces with features of dimension {x_2.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define binary labels\n",
    "We retrieve the labels associated to the nodes of each input simplex. In the KarateClub dataset, two social groups emerge. So we assign binary labels to the nodes indicating of which group they are a part.\n",
    "\n",
    "We convert the binary labels into one-hot encoder form, and keep the last four nodes' true labels for the purpose of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(\n",
    "    [\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "y_true = np.zeros((34, 2))\n",
    "y_true[:, 0] = y\n",
    "y_true[:, 1] = 1 - y\n",
    "y_test = y_true[-4:]\n",
    "y_train = y_true[:30]\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the HSNLayer class, we create a neural network with stacked layers. A linear layer at the end produces an output with shape $n_\\text{nodes} \\times 2$, so we can compare with our binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSN(torch.nn.Module):\n",
    "    \"\"\"High Skip Network Implementation for binary node classification.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    channels : int\n",
    "        Dimension of features\n",
    "    n_layers : int\n",
    "        Amount of message passing layers.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, n_layers=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(\n",
    "                HSNLayer(\n",
    "                    channels=channels,\n",
    "                )\n",
    "            )\n",
    "        self.linear = torch.nn.Linear(channels, 2)\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x_0, incidence_1, adjacency_0):\n",
    "        \"\"\"Forward computation.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        x_0 : tensor\n",
    "            shape = [n_nodes, channels]\n",
    "            Node features.\n",
    "\n",
    "        incidence_1 : tensor\n",
    "            shape = [n_nodes, n_edges]\n",
    "            Boundary matrix of rank 1.\n",
    "\n",
    "        adjacency_0 : tensor\n",
    "            shape = [n_nodes, n_nodes]\n",
    "            Adjacency matrix (up) of rank 0.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        _ : tensor\n",
    "            shape = [n_nodes, 2]\n",
    "            One-hot labels assigned to nodes.\n",
    "\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x_0 = layer(x_0, incidence_1, adjacency_0)\n",
    "        logits = self.linear(x_0)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model with our pre-made neighborhood structures and specify an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HSN(\n",
    "    channels=channels_nodes,\n",
    "    n_layers=10,\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSN(\n",
      "  (linear): Linear(in_features=8, out_features=2, bias=True)\n",
      "  (layers): ModuleList(\n",
      "    (0-9): 10 x HSNLayer(\n",
      "      (conv_level1_0_to_0): Conv()\n",
      "      (conv_level1_0_to_1): Conv()\n",
      "      (conv_level2_0_to_0): Conv()\n",
      "      (conv_level2_1_to_0): Conv()\n",
      "      (aggr_on_nodes): Aggregation()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs the training, looping over the network for a low number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.7293 Train_acc: 0.1667\n",
      "Epoch: 2 loss: 0.6926 Train_acc: 0.0000\n",
      "Epoch: 3 loss: 0.6711 Train_acc: 0.0000\n",
      "Epoch: 4 loss: 0.6475 Train_acc: 0.0667\n",
      "Epoch: 5 loss: 0.6277 Train_acc: 0.3000\n",
      "Epoch: 6 loss: 0.6088 Train_acc: 0.3333\n",
      "Epoch: 7 loss: 0.5909 Train_acc: 0.3333\n",
      "Epoch: 8 loss: 0.5681 Train_acc: 0.3667\n",
      "Epoch: 9 loss: 0.5495 Train_acc: 0.4333\n",
      "Epoch: 10 loss: 0.5237 Train_acc: 0.4333\n",
      "Epoch: 11 loss: 0.4991 Train_acc: 0.5333\n",
      "Epoch: 12 loss: 0.4753 Train_acc: 0.5333\n",
      "Epoch: 13 loss: 0.4512 Train_acc: 0.5333\n",
      "Epoch: 14 loss: 0.4269 Train_acc: 0.5333\n",
      "Epoch: 15 loss: 0.4111 Train_acc: 0.5333\n",
      "Epoch: 16 loss: 0.3984 Train_acc: 0.5333\n",
      "Epoch: 17 loss: 0.3787 Train_acc: 0.5000\n",
      "Epoch: 18 loss: 0.3638 Train_acc: 0.5000\n",
      "Epoch: 19 loss: 0.3521 Train_acc: 0.5333\n",
      "Epoch: 20 loss: 0.3374 Train_acc: 0.5333\n",
      "Epoch: 21 loss: 0.3295 Train_acc: 0.5667\n",
      "Epoch: 22 loss: 0.3166 Train_acc: 0.5667\n",
      "Epoch: 23 loss: 0.3052 Train_acc: 0.5667\n",
      "Epoch: 24 loss: 0.2985 Train_acc: 0.5667\n",
      "Epoch: 25 loss: 0.2934 Train_acc: 0.5667\n",
      "Epoch: 26 loss: 0.2880 Train_acc: 0.5667\n",
      "Epoch: 27 loss: 0.2805 Train_acc: 0.6000\n",
      "Epoch: 28 loss: 0.2690 Train_acc: 0.6000\n",
      "Epoch: 29 loss: 0.2511 Train_acc: 0.6000\n",
      "Epoch: 30 loss: 0.2419 Train_acc: 0.6333\n",
      "Epoch: 31 loss: 0.2350 Train_acc: 0.6333\n",
      "Epoch: 32 loss: 0.2246 Train_acc: 0.6333\n",
      "Epoch: 33 loss: 0.2226 Train_acc: 0.6333\n",
      "Epoch: 34 loss: 0.2175 Train_acc: 0.6333\n",
      "Epoch: 35 loss: 0.2155 Train_acc: 0.6333\n",
      "Epoch: 36 loss: 0.2082 Train_acc: 0.6333\n",
      "Epoch: 37 loss: 0.2033 Train_acc: 0.6333\n",
      "Epoch: 38 loss: 0.2017 Train_acc: 0.6333\n",
      "Epoch: 39 loss: 0.2688 Train_acc: 0.5667\n",
      "Epoch: 40 loss: 0.2657 Train_acc: 0.5667\n",
      "Epoch: 41 loss: 0.2590 Train_acc: 0.5667\n",
      "Epoch: 42 loss: 0.2435 Train_acc: 0.6000\n",
      "Epoch: 43 loss: 0.2490 Train_acc: 0.6000\n",
      "Epoch: 44 loss: 0.2131 Train_acc: 0.6000\n",
      "Epoch: 45 loss: 0.2091 Train_acc: 0.6333\n",
      "Epoch: 46 loss: 0.2140 Train_acc: 0.6000\n",
      "Epoch: 47 loss: 0.1872 Train_acc: 0.6333\n",
      "Epoch: 48 loss: 0.1991 Train_acc: 0.6333\n",
      "Epoch: 49 loss: 0.2465 Train_acc: 0.6000\n",
      "Epoch: 50 loss: 0.2040 Train_acc: 0.6333\n",
      "Epoch: 51 loss: 0.1922 Train_acc: 0.6333\n",
      "Epoch: 52 loss: 0.2375 Train_acc: 0.6333\n",
      "Epoch: 53 loss: 0.1890 Train_acc: 0.6333\n",
      "Epoch: 54 loss: 0.2047 Train_acc: 0.6000\n",
      "Epoch: 55 loss: 0.1978 Train_acc: 0.6333\n",
      "Epoch: 56 loss: 0.2499 Train_acc: 0.5667\n",
      "Epoch: 57 loss: 0.1479 Train_acc: 0.6000\n",
      "Epoch: 58 loss: 0.7192 Train_acc: 0.5333\n",
      "Epoch: 59 loss: 0.1348 Train_acc: 0.6000\n",
      "Epoch: 60 loss: 0.1812 Train_acc: 0.6000\n",
      "Epoch: 61 loss: 0.2068 Train_acc: 0.5333\n",
      "Epoch: 62 loss: 0.1249 Train_acc: 0.6000\n",
      "Epoch: 63 loss: 0.1200 Train_acc: 0.6000\n",
      "Epoch: 64 loss: 0.1187 Train_acc: 0.6667\n",
      "Epoch: 65 loss: 0.1102 Train_acc: 0.6333\n",
      "Epoch: 66 loss: 0.1203 Train_acc: 0.6000\n",
      "Epoch: 67 loss: 0.1014 Train_acc: 0.6667\n",
      "Epoch: 68 loss: 0.0856 Train_acc: 0.6333\n",
      "Epoch: 69 loss: 0.0685 Train_acc: 0.6333\n",
      "Epoch: 70 loss: 0.0640 Train_acc: 0.6000\n",
      "Epoch: 71 loss: 0.0629 Train_acc: 0.6000\n",
      "Epoch: 72 loss: 0.0541 Train_acc: 0.6000\n",
      "Epoch: 73 loss: 0.0469 Train_acc: 0.6333\n",
      "Epoch: 74 loss: 0.0376 Train_acc: 0.6333\n",
      "Epoch: 75 loss: 0.0369 Train_acc: 0.6333\n",
      "Epoch: 76 loss: 0.0361 Train_acc: 0.6333\n",
      "Epoch: 77 loss: 0.0326 Train_acc: 0.6333\n",
      "Epoch: 78 loss: 0.0281 Train_acc: 0.6333\n",
      "Epoch: 79 loss: 0.0249 Train_acc: 0.6333\n",
      "Epoch: 80 loss: 0.0229 Train_acc: 0.6333\n",
      "Epoch: 81 loss: 0.0218 Train_acc: 0.6333\n",
      "Epoch: 82 loss: 0.0207 Train_acc: 0.6333\n",
      "Epoch: 83 loss: 0.0195 Train_acc: 0.6333\n",
      "Epoch: 84 loss: 0.0182 Train_acc: 0.6333\n",
      "Epoch: 85 loss: 0.0168 Train_acc: 0.6333\n",
      "Epoch: 86 loss: 0.0155 Train_acc: 0.6333\n",
      "Epoch: 87 loss: 0.0142 Train_acc: 0.6333\n",
      "Epoch: 88 loss: 0.0131 Train_acc: 0.6333\n",
      "Epoch: 89 loss: 0.0121 Train_acc: 0.6333\n",
      "Epoch: 90 loss: 0.0112 Train_acc: 0.6333\n",
      "Epoch: 91 loss: 0.0105 Train_acc: 0.6333\n",
      "Epoch: 92 loss: 0.0099 Train_acc: 0.6333\n",
      "Epoch: 93 loss: 0.0095 Train_acc: 0.6333\n",
      "Epoch: 94 loss: 0.0090 Train_acc: 0.6333\n",
      "Epoch: 95 loss: 0.0085 Train_acc: 0.6333\n",
      "Epoch: 96 loss: 0.0079 Train_acc: 0.6333\n",
      "Epoch: 97 loss: 0.0074 Train_acc: 0.6333\n",
      "Epoch: 98 loss: 0.0071 Train_acc: 0.6333\n",
      "Epoch: 99 loss: 0.0068 Train_acc: 0.6333\n",
      "Epoch: 100 loss: 0.0065 Train_acc: 0.6333\n",
      "Test_acc: 0.2500\n",
      "Epoch: 101 loss: 0.0063 Train_acc: 0.6333\n",
      "Epoch: 102 loss: 0.0061 Train_acc: 0.6333\n",
      "Epoch: 103 loss: 0.0060 Train_acc: 0.6333\n",
      "Epoch: 104 loss: 0.0058 Train_acc: 0.6333\n",
      "Epoch: 105 loss: 0.0056 Train_acc: 0.6333\n",
      "Epoch: 106 loss: 0.0054 Train_acc: 0.6333\n",
      "Epoch: 107 loss: 0.0052 Train_acc: 0.6333\n",
      "Epoch: 108 loss: 0.0050 Train_acc: 0.6333\n",
      "Epoch: 109 loss: 0.0048 Train_acc: 0.6333\n",
      "Epoch: 110 loss: 0.0046 Train_acc: 0.6333\n",
      "Epoch: 111 loss: 0.0043 Train_acc: 0.6333\n",
      "Epoch: 112 loss: 0.0041 Train_acc: 0.6333\n",
      "Epoch: 113 loss: 0.0039 Train_acc: 0.6333\n",
      "Epoch: 114 loss: 0.0037 Train_acc: 0.6333\n",
      "Epoch: 115 loss: 0.0036 Train_acc: 0.6333\n",
      "Epoch: 116 loss: 0.0035 Train_acc: 0.6333\n",
      "Epoch: 117 loss: 0.0034 Train_acc: 0.6333\n",
      "Epoch: 118 loss: 0.0033 Train_acc: 0.6333\n",
      "Epoch: 119 loss: 0.0032 Train_acc: 0.6333\n",
      "Epoch: 120 loss: 0.0032 Train_acc: 0.6333\n",
      "Epoch: 121 loss: 0.0031 Train_acc: 0.6333\n",
      "Epoch: 122 loss: 0.0030 Train_acc: 0.6333\n",
      "Epoch: 123 loss: 0.0030 Train_acc: 0.6333\n",
      "Epoch: 124 loss: 0.0029 Train_acc: 0.6333\n",
      "Epoch: 125 loss: 0.0029 Train_acc: 0.6333\n",
      "Epoch: 126 loss: 0.0028 Train_acc: 0.6333\n",
      "Epoch: 127 loss: 0.0027 Train_acc: 0.6333\n",
      "Epoch: 128 loss: 0.0027 Train_acc: 0.6333\n",
      "Epoch: 129 loss: 0.0027 Train_acc: 0.6333\n",
      "Epoch: 130 loss: 0.0026 Train_acc: 0.6333\n",
      "Epoch: 131 loss: 0.0026 Train_acc: 0.6333\n",
      "Epoch: 132 loss: 0.0025 Train_acc: 0.6333\n",
      "Epoch: 133 loss: 0.0025 Train_acc: 0.6333\n",
      "Epoch: 134 loss: 0.0024 Train_acc: 0.6333\n",
      "Epoch: 135 loss: 0.0024 Train_acc: 0.6333\n",
      "Epoch: 136 loss: 0.0024 Train_acc: 0.6333\n",
      "Epoch: 137 loss: 0.0024 Train_acc: 0.6333\n",
      "Epoch: 138 loss: 0.0023 Train_acc: 0.6333\n",
      "Epoch: 139 loss: 0.0023 Train_acc: 0.6333\n",
      "Epoch: 140 loss: 0.0023 Train_acc: 0.6333\n",
      "Epoch: 141 loss: 0.0022 Train_acc: 0.6333\n",
      "Epoch: 142 loss: 0.0022 Train_acc: 0.6333\n",
      "Epoch: 143 loss: 0.0022 Train_acc: 0.6333\n",
      "Epoch: 144 loss: 0.0022 Train_acc: 0.6333\n",
      "Epoch: 145 loss: 0.0022 Train_acc: 0.6333\n",
      "Epoch: 146 loss: 0.0021 Train_acc: 0.6333\n",
      "Epoch: 147 loss: 0.0021 Train_acc: 0.6333\n",
      "Epoch: 148 loss: 0.0021 Train_acc: 0.6333\n",
      "Epoch: 149 loss: 0.0021 Train_acc: 0.6333\n",
      "Epoch: 150 loss: 0.0020 Train_acc: 0.6333\n",
      "Epoch: 151 loss: 0.0020 Train_acc: 0.6333\n",
      "Epoch: 152 loss: 0.0020 Train_acc: 0.6333\n",
      "Epoch: 153 loss: 0.0020 Train_acc: 0.6333\n",
      "Epoch: 154 loss: 0.0020 Train_acc: 0.6333\n",
      "Epoch: 155 loss: 0.0019 Train_acc: 0.6333\n",
      "Epoch: 156 loss: 0.0019 Train_acc: 0.6333\n",
      "Epoch: 157 loss: 0.0019 Train_acc: 0.6333\n",
      "Epoch: 158 loss: 0.0019 Train_acc: 0.6333\n",
      "Epoch: 159 loss: 0.0019 Train_acc: 0.6333\n",
      "Epoch: 160 loss: 0.0018 Train_acc: 0.6333\n",
      "Epoch: 161 loss: 0.0018 Train_acc: 0.6333\n",
      "Epoch: 162 loss: 0.0018 Train_acc: 0.6333\n",
      "Epoch: 163 loss: 0.0018 Train_acc: 0.6333\n",
      "Epoch: 164 loss: 0.0018 Train_acc: 0.6333\n",
      "Epoch: 165 loss: 0.0018 Train_acc: 0.6333\n",
      "Epoch: 166 loss: 0.0017 Train_acc: 0.6333\n",
      "Epoch: 167 loss: 0.0017 Train_acc: 0.6333\n",
      "Epoch: 168 loss: 0.0017 Train_acc: 0.6333\n",
      "Epoch: 169 loss: 0.0017 Train_acc: 0.6333\n",
      "Epoch: 170 loss: 0.0017 Train_acc: 0.6333\n",
      "Epoch: 171 loss: 0.0017 Train_acc: 0.6333\n",
      "Epoch: 172 loss: 0.0016 Train_acc: 0.6333\n",
      "Epoch: 173 loss: 0.0016 Train_acc: 0.6333\n",
      "Epoch: 174 loss: 0.0016 Train_acc: 0.6333\n",
      "Epoch: 175 loss: 0.0016 Train_acc: 0.6333\n",
      "Epoch: 176 loss: 0.0016 Train_acc: 0.6333\n",
      "Epoch: 177 loss: 0.0016 Train_acc: 0.6333\n",
      "Epoch: 178 loss: 0.0016 Train_acc: 0.6333\n",
      "Epoch: 179 loss: 0.0016 Train_acc: 0.6333\n",
      "Epoch: 180 loss: 0.0015 Train_acc: 0.6333\n",
      "Epoch: 181 loss: 0.0015 Train_acc: 0.6333\n",
      "Epoch: 182 loss: 0.0015 Train_acc: 0.6333\n",
      "Epoch: 183 loss: 0.0015 Train_acc: 0.6333\n",
      "Epoch: 184 loss: 0.0015 Train_acc: 0.6333\n",
      "Epoch: 185 loss: 0.0015 Train_acc: 0.6333\n",
      "Epoch: 186 loss: 0.0015 Train_acc: 0.6333\n",
      "Epoch: 187 loss: 0.0015 Train_acc: 0.6333\n",
      "Epoch: 188 loss: 0.0014 Train_acc: 0.6333\n",
      "Epoch: 189 loss: 0.0014 Train_acc: 0.6333\n",
      "Epoch: 190 loss: 0.0014 Train_acc: 0.6333\n",
      "Epoch: 191 loss: 0.0014 Train_acc: 0.6333\n",
      "Epoch: 192 loss: 0.0014 Train_acc: 0.6333\n",
      "Epoch: 193 loss: 0.0014 Train_acc: 0.6333\n",
      "Epoch: 194 loss: 0.0014 Train_acc: 0.6333\n",
      "Epoch: 195 loss: 0.0014 Train_acc: 0.6333\n",
      "Epoch: 196 loss: 0.0014 Train_acc: 0.6333\n",
      "Epoch: 197 loss: 0.0014 Train_acc: 0.6333\n",
      "Epoch: 198 loss: 0.0013 Train_acc: 0.6333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 199 loss: 0.0013 Train_acc: 0.6333\n",
      "Epoch: 200 loss: 0.0013 Train_acc: 0.6333\n",
      "Test_acc: 0.2500\n",
      "Epoch: 201 loss: 0.0013 Train_acc: 0.6333\n",
      "Epoch: 202 loss: 0.0013 Train_acc: 0.6333\n",
      "Epoch: 203 loss: 0.0013 Train_acc: 0.6333\n",
      "Epoch: 204 loss: 0.0013 Train_acc: 0.6333\n",
      "Epoch: 205 loss: 0.0013 Train_acc: 0.6333\n",
      "Epoch: 206 loss: 0.0013 Train_acc: 0.6333\n",
      "Epoch: 207 loss: 0.0013 Train_acc: 0.6333\n",
      "Epoch: 208 loss: 0.0013 Train_acc: 0.6333\n",
      "Epoch: 209 loss: 0.0012 Train_acc: 0.6333\n",
      "Epoch: 210 loss: 0.0012 Train_acc: 0.6333\n",
      "Epoch: 211 loss: 0.0012 Train_acc: 0.6333\n",
      "Epoch: 212 loss: 0.0012 Train_acc: 0.6333\n",
      "Epoch: 213 loss: 0.0012 Train_acc: 0.6333\n",
      "Epoch: 214 loss: 0.0012 Train_acc: 0.6333\n",
      "Epoch: 215 loss: 0.0012 Train_acc: 0.6333\n",
      "Epoch: 216 loss: 0.0012 Train_acc: 0.6333\n",
      "Epoch: 217 loss: 0.0012 Train_acc: 0.6333\n",
      "Epoch: 218 loss: 0.0012 Train_acc: 0.6333\n",
      "Epoch: 219 loss: 0.0012 Train_acc: 0.6333\n",
      "Epoch: 220 loss: 0.0012 Train_acc: 0.6333\n",
      "Epoch: 221 loss: 0.0011 Train_acc: 0.6333\n",
      "Epoch: 222 loss: 0.0011 Train_acc: 0.6333\n",
      "Epoch: 223 loss: 0.0011 Train_acc: 0.6333\n",
      "Epoch: 224 loss: 0.0011 Train_acc: 0.6333\n",
      "Epoch: 225 loss: 0.0011 Train_acc: 0.6333\n",
      "Epoch: 226 loss: 0.0011 Train_acc: 0.6333\n",
      "Epoch: 227 loss: 0.0011 Train_acc: 0.6333\n",
      "Epoch: 228 loss: 0.0011 Train_acc: 0.6333\n",
      "Epoch: 229 loss: 0.0011 Train_acc: 0.6333\n",
      "Epoch: 230 loss: 0.0011 Train_acc: 0.6333\n",
      "Epoch: 231 loss: 0.0011 Train_acc: 0.6333\n",
      "Epoch: 232 loss: 0.0011 Train_acc: 0.6333\n",
      "Epoch: 233 loss: 0.0011 Train_acc: 0.6333\n",
      "Epoch: 234 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 235 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 236 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 237 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 238 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 239 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 240 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 241 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 242 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 243 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 244 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 245 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 246 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 247 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 248 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 249 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 250 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 251 loss: 0.0010 Train_acc: 0.6333\n",
      "Epoch: 252 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 253 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 254 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 255 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 256 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 257 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 258 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 259 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 260 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 261 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 262 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 263 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 264 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 265 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 266 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 267 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 268 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 269 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 270 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 271 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 272 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 273 loss: 0.0009 Train_acc: 0.6333\n",
      "Epoch: 274 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 275 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 276 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 277 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 278 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 279 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 280 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 281 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 282 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 283 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 284 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 285 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 286 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 287 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 288 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 289 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 290 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 291 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 292 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 293 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 294 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 295 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 296 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 297 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 298 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 299 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 300 loss: 0.0008 Train_acc: 0.6333\n",
      "Test_acc: 0.2500\n",
      "Epoch: 301 loss: 0.0008 Train_acc: 0.6333\n",
      "Epoch: 302 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 303 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 304 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 305 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 306 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 307 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 308 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 309 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 310 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 311 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 312 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 313 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 314 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 315 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 316 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 317 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 318 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 319 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 320 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 321 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 322 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 323 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 324 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 325 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 326 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 327 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 328 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 329 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 330 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 331 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 332 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 333 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 334 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 335 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 336 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 337 loss: 0.0007 Train_acc: 0.6333\n",
      "Epoch: 338 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 339 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 340 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 341 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 342 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 343 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 344 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 345 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 346 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 347 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 348 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 349 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 350 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 351 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 352 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 353 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 354 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 355 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 356 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 357 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 358 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 359 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 360 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 361 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 362 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 363 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 364 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 365 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 366 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 367 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 368 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 369 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 370 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 371 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 372 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 373 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 374 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 375 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 376 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 377 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 378 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 379 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 380 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 381 loss: 0.0006 Train_acc: 0.6333\n",
      "Epoch: 382 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 383 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 384 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 385 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 386 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 387 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 388 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 389 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 390 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 391 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 392 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 393 loss: 0.0005 Train_acc: 0.6333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 394 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 395 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 396 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 397 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 398 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 399 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 400 loss: 0.0005 Train_acc: 0.6333\n",
      "Test_acc: 0.2500\n",
      "Epoch: 401 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 402 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 403 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 404 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 405 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 406 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 407 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 408 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 409 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 410 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 411 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 412 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 413 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 414 loss: 0.0005 Train_acc: 0.6333\n",
      "Epoch: 415 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 416 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 417 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 418 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 419 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 420 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 421 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 422 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 423 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 424 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 425 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 426 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 427 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 428 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 429 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 430 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 431 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 432 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 433 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 434 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 435 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 436 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 437 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 438 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 439 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 440 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 441 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 442 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 443 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 444 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 445 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 446 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 447 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 448 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 449 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 450 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 451 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 452 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 453 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 454 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 455 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 456 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 457 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 458 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 459 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 460 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 461 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 462 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 463 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 464 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 465 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 466 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 467 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 468 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 469 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 470 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 471 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 472 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 473 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 474 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 475 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 476 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 477 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 478 loss: 0.0004 Train_acc: 0.6333\n",
      "Epoch: 479 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 480 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 481 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 482 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 483 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 484 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 485 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 486 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 487 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 488 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 489 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 490 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 491 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 492 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 493 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 494 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 495 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 496 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 497 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 498 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 499 loss: 0.0003 Train_acc: 0.6333\n",
      "Epoch: 500 loss: 0.0003 Train_acc: 0.6333\n",
      "Test_acc: 0.2500\n"
     ]
    }
   ],
   "source": [
    "test_interval = 100\n",
    "num_epochs = 500\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_hat = model(x_0, incidence_1, adjacency_0)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        y_hat[: len(y_train)].float(), y_train.float()\n",
    "    )\n",
    "    epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.where(y_hat > 0.5, torch.tensor(1), torch.tensor(0))\n",
    "    accuracy = (y_pred[-len(y_train) :] == y_train).all(dim=1).float().mean().item()\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {accuracy:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            y_hat_test = model(x_0, incidence_1, adjacency_0)\n",
    "            y_pred_test = torch.where(\n",
    "                y_hat_test > 0.5, torch.tensor(1), torch.tensor(0)\n",
    "            )\n",
    "            test_accuracy = (\n",
    "                torch.eq(y_pred_test[-len(y_test) :], y_test)\n",
    "                .all(dim=1)\n",
    "                .float()\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "            print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
